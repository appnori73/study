{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appnori73/study/blob/master/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flmWi8ltb9S0",
        "colab_type": "code",
        "outputId": "b6f65985-2cb1-4457-861c-aae3fa0105a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6719
        }
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Reshape\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from keras.layers import LeakyReLU, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
        "\n",
        "class DCGAN(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "\n",
        "    # (Wâˆ’F+2P)/S+1\n",
        "    def discriminator(self):\n",
        "        if self.D:\n",
        "            return self.D\n",
        "        self.D = Sequential()\n",
        "        depth = 64\n",
        "        dropout = 0.4\n",
        "        # In: 28 x 28 x 1, depth = 1\n",
        "        # Out: 14 x 14 x 1, depth=64\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n",
        "            padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Out: 1-dim probability\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1,acitvation='sigmoid'))\n",
        "        self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    def generator(self):\n",
        "        if self.G:\n",
        "            return self.G\n",
        "        self.G = Sequential()\n",
        "        dropout = 0.4\n",
        "        depth = 64+64+64+64\n",
        "        dim = 7\n",
        "        # In: 100\n",
        "        # Out: dim x dim x depth\n",
        "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((dim, dim, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "\n",
        "        # In: dim x dim x depth\n",
        "        # Out: 2*dim x 2*dim x depth/2\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
        "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        self.G.summary()\n",
        "        return self.G\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.DM\n",
        "\n",
        "    def adversarial_model(self):\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.AM\n",
        "\n",
        "class MNIST_DCGAN(object):\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channel = 1\n",
        "\n",
        "        self.x_train = input_data.read_data_sets(\"mnist\",\\\n",
        "        \tone_hot=True).train.images\n",
        "        self.x_train = self.x_train.reshape(-1, self.img_rows,\\\n",
        "        \tself.img_cols, 1).astype(np.float32)\n",
        "\n",
        "        self.DCGAN = DCGAN()\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "        noise_input = None\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "        for i in range(train_steps):\n",
        "            images_train = self.x_train[np.random.randint(0,\n",
        "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            images_fake = self.generator.predict(noise)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            y = np.ones([batch_size, 1])\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
        "                        noise=noise_input, step=(i+1))\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
        "        filename = 'mnist.png'\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                filename = \"mnist_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mnist_dcgan = MNIST_DCGAN()\n",
        "    timer = ElapsedTimer()\n",
        "    mnist_dcgan.train(train_steps=200, batch_size=64, save_interval=100)\n",
        "    timer.elapsed_time()\n",
        "    mnist_dcgan.plot_images(fake=True)\n",
        "    mnist_dcgan.plot_images(fake=False, save2file=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-622a505c551b>:134: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 8193      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 4,311,553\n",
            "Trainable params: 4,311,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 12544)             1266944   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,394,241\n",
            "Trainable params: 2,368,705\n",
            "Non-trainable params: 25,536\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "0: [D loss: 0.691626, acc: 0.546875]  [A loss: 0.968613, acc: 0.000000]\n",
            "1: [D loss: 0.671124, acc: 0.507812]  [A loss: 0.895417, acc: 0.000000]\n",
            "2: [D loss: 0.621352, acc: 1.000000]  [A loss: 0.973360, acc: 0.000000]\n",
            "3: [D loss: 0.542979, acc: 0.796875]  [A loss: 1.258935, acc: 0.000000]\n",
            "4: [D loss: 0.506081, acc: 0.984375]  [A loss: 0.807990, acc: 0.140625]\n",
            "5: [D loss: 0.895276, acc: 0.500000]  [A loss: 1.418844, acc: 0.000000]\n",
            "6: [D loss: 0.472270, acc: 0.960938]  [A loss: 1.075944, acc: 0.000000]\n",
            "7: [D loss: 0.437277, acc: 0.984375]  [A loss: 1.055148, acc: 0.000000]\n",
            "8: [D loss: 0.455618, acc: 0.609375]  [A loss: 1.328248, acc: 0.000000]\n",
            "9: [D loss: 0.468926, acc: 0.601562]  [A loss: 1.472779, acc: 0.000000]\n",
            "10: [D loss: 0.436100, acc: 0.835938]  [A loss: 1.386913, acc: 0.000000]\n",
            "11: [D loss: 0.418607, acc: 0.679688]  [A loss: 1.649808, acc: 0.000000]\n",
            "12: [D loss: 0.385289, acc: 1.000000]  [A loss: 1.307582, acc: 0.000000]\n",
            "13: [D loss: 0.448705, acc: 0.546875]  [A loss: 1.861238, acc: 0.000000]\n",
            "14: [D loss: 0.372492, acc: 0.992188]  [A loss: 1.146120, acc: 0.000000]\n",
            "15: [D loss: 0.520962, acc: 0.507812]  [A loss: 1.917956, acc: 0.000000]\n",
            "16: [D loss: 0.362686, acc: 0.992188]  [A loss: 1.196526, acc: 0.000000]\n",
            "17: [D loss: 0.394041, acc: 0.640625]  [A loss: 1.701569, acc: 0.000000]\n",
            "18: [D loss: 0.247689, acc: 1.000000]  [A loss: 1.469732, acc: 0.000000]\n",
            "19: [D loss: 0.331946, acc: 0.875000]  [A loss: 1.830156, acc: 0.000000]\n",
            "20: [D loss: 0.244453, acc: 0.992188]  [A loss: 1.554884, acc: 0.000000]\n",
            "21: [D loss: 0.346714, acc: 0.781250]  [A loss: 2.198726, acc: 0.000000]\n",
            "22: [D loss: 0.269133, acc: 0.992188]  [A loss: 1.290926, acc: 0.000000]\n",
            "23: [D loss: 0.532862, acc: 0.539062]  [A loss: 2.396826, acc: 0.000000]\n",
            "24: [D loss: 0.364000, acc: 0.921875]  [A loss: 1.205207, acc: 0.000000]\n",
            "25: [D loss: 0.349542, acc: 0.773438]  [A loss: 1.649339, acc: 0.000000]\n",
            "26: [D loss: 0.227545, acc: 1.000000]  [A loss: 1.513193, acc: 0.000000]\n",
            "27: [D loss: 0.315597, acc: 0.835938]  [A loss: 2.015492, acc: 0.000000]\n",
            "28: [D loss: 0.209432, acc: 1.000000]  [A loss: 1.576646, acc: 0.000000]\n",
            "29: [D loss: 0.364886, acc: 0.750000]  [A loss: 2.274009, acc: 0.000000]\n",
            "30: [D loss: 0.239962, acc: 0.976562]  [A loss: 1.302763, acc: 0.015625]\n",
            "31: [D loss: 0.520203, acc: 0.546875]  [A loss: 2.373726, acc: 0.000000]\n",
            "32: [D loss: 0.319100, acc: 0.929688]  [A loss: 1.131552, acc: 0.015625]\n",
            "33: [D loss: 0.534418, acc: 0.601562]  [A loss: 2.023386, acc: 0.000000]\n",
            "34: [D loss: 0.228833, acc: 0.992188]  [A loss: 1.403747, acc: 0.000000]\n",
            "35: [D loss: 0.319383, acc: 0.835938]  [A loss: 1.800120, acc: 0.000000]\n",
            "36: [D loss: 0.246668, acc: 0.968750]  [A loss: 1.760208, acc: 0.000000]\n",
            "37: [D loss: 0.257166, acc: 0.953125]  [A loss: 1.867725, acc: 0.000000]\n",
            "38: [D loss: 0.258804, acc: 0.976562]  [A loss: 1.582941, acc: 0.000000]\n",
            "39: [D loss: 0.388299, acc: 0.750000]  [A loss: 2.447572, acc: 0.000000]\n",
            "40: [D loss: 0.371709, acc: 0.882812]  [A loss: 0.747187, acc: 0.484375]\n",
            "41: [D loss: 1.053379, acc: 0.500000]  [A loss: 2.183944, acc: 0.000000]\n",
            "42: [D loss: 0.290564, acc: 0.937500]  [A loss: 1.238206, acc: 0.062500]\n",
            "43: [D loss: 0.394571, acc: 0.757812]  [A loss: 1.693932, acc: 0.000000]\n",
            "44: [D loss: 0.293046, acc: 0.929688]  [A loss: 1.480339, acc: 0.000000]\n",
            "45: [D loss: 0.431991, acc: 0.734375]  [A loss: 1.886202, acc: 0.000000]\n",
            "46: [D loss: 0.327759, acc: 0.929688]  [A loss: 1.432516, acc: 0.000000]\n",
            "47: [D loss: 0.441391, acc: 0.679688]  [A loss: 2.170862, acc: 0.000000]\n",
            "48: [D loss: 0.354049, acc: 0.960938]  [A loss: 1.102162, acc: 0.078125]\n",
            "49: [D loss: 0.634069, acc: 0.523438]  [A loss: 2.318754, acc: 0.000000]\n",
            "50: [D loss: 0.483056, acc: 0.835938]  [A loss: 0.726062, acc: 0.484375]\n",
            "51: [D loss: 0.902169, acc: 0.500000]  [A loss: 1.891152, acc: 0.000000]\n",
            "52: [D loss: 0.376701, acc: 0.937500]  [A loss: 1.089761, acc: 0.046875]\n",
            "53: [D loss: 0.526577, acc: 0.578125]  [A loss: 1.696733, acc: 0.000000]\n",
            "54: [D loss: 0.387340, acc: 0.914062]  [A loss: 1.146313, acc: 0.000000]\n",
            "55: [D loss: 0.529824, acc: 0.570312]  [A loss: 1.766991, acc: 0.000000]\n",
            "56: [D loss: 0.425885, acc: 0.851562]  [A loss: 0.997063, acc: 0.140625]\n",
            "57: [D loss: 0.635645, acc: 0.507812]  [A loss: 2.091016, acc: 0.000000]\n",
            "58: [D loss: 0.410238, acc: 0.906250]  [A loss: 0.894966, acc: 0.187500]\n",
            "59: [D loss: 0.670021, acc: 0.546875]  [A loss: 2.009120, acc: 0.000000]\n",
            "60: [D loss: 0.438531, acc: 0.882812]  [A loss: 0.917552, acc: 0.156250]\n",
            "61: [D loss: 0.665524, acc: 0.492188]  [A loss: 1.730018, acc: 0.000000]\n",
            "62: [D loss: 0.416100, acc: 0.875000]  [A loss: 1.074213, acc: 0.062500]\n",
            "63: [D loss: 0.571964, acc: 0.539062]  [A loss: 1.896755, acc: 0.000000]\n",
            "64: [D loss: 0.381027, acc: 0.960938]  [A loss: 1.092295, acc: 0.078125]\n",
            "65: [D loss: 0.613157, acc: 0.523438]  [A loss: 2.021575, acc: 0.000000]\n",
            "66: [D loss: 0.422631, acc: 0.929688]  [A loss: 1.022402, acc: 0.062500]\n",
            "67: [D loss: 0.606272, acc: 0.523438]  [A loss: 1.959731, acc: 0.000000]\n",
            "68: [D loss: 0.443075, acc: 0.890625]  [A loss: 0.853444, acc: 0.187500]\n",
            "69: [D loss: 0.728545, acc: 0.507812]  [A loss: 2.041230, acc: 0.000000]\n",
            "70: [D loss: 0.475901, acc: 0.875000]  [A loss: 0.793312, acc: 0.250000]\n",
            "71: [D loss: 0.726751, acc: 0.507812]  [A loss: 1.793105, acc: 0.000000]\n",
            "72: [D loss: 0.446367, acc: 0.835938]  [A loss: 1.077061, acc: 0.046875]\n",
            "73: [D loss: 0.548953, acc: 0.570312]  [A loss: 1.728436, acc: 0.000000]\n",
            "74: [D loss: 0.429748, acc: 0.882812]  [A loss: 1.209081, acc: 0.015625]\n",
            "75: [D loss: 0.531822, acc: 0.609375]  [A loss: 1.756965, acc: 0.000000]\n",
            "76: [D loss: 0.462474, acc: 0.773438]  [A loss: 1.351594, acc: 0.000000]\n",
            "77: [D loss: 0.495709, acc: 0.632812]  [A loss: 1.812920, acc: 0.000000]\n",
            "78: [D loss: 0.423970, acc: 0.828125]  [A loss: 1.319524, acc: 0.015625]\n",
            "79: [D loss: 0.588204, acc: 0.578125]  [A loss: 2.391459, acc: 0.000000]\n",
            "80: [D loss: 0.545820, acc: 0.773438]  [A loss: 0.454671, acc: 0.937500]\n",
            "81: [D loss: 1.049501, acc: 0.500000]  [A loss: 1.957946, acc: 0.000000]\n",
            "82: [D loss: 0.490768, acc: 0.875000]  [A loss: 0.773388, acc: 0.359375]\n",
            "83: [D loss: 0.721701, acc: 0.507812]  [A loss: 1.667687, acc: 0.000000]\n",
            "84: [D loss: 0.494470, acc: 0.859375]  [A loss: 0.918649, acc: 0.171875]\n",
            "85: [D loss: 0.638027, acc: 0.507812]  [A loss: 1.765787, acc: 0.000000]\n",
            "86: [D loss: 0.511602, acc: 0.820312]  [A loss: 0.926635, acc: 0.156250]\n",
            "87: [D loss: 0.641039, acc: 0.500000]  [A loss: 1.904048, acc: 0.000000]\n",
            "88: [D loss: 0.522517, acc: 0.804688]  [A loss: 0.694814, acc: 0.546875]\n",
            "89: [D loss: 0.769268, acc: 0.500000]  [A loss: 1.947028, acc: 0.000000]\n",
            "90: [D loss: 0.545414, acc: 0.789062]  [A loss: 0.670252, acc: 0.609375]\n",
            "91: [D loss: 0.811098, acc: 0.500000]  [A loss: 1.732436, acc: 0.000000]\n",
            "92: [D loss: 0.482732, acc: 0.914062]  [A loss: 0.761323, acc: 0.343750]\n",
            "93: [D loss: 0.658647, acc: 0.500000]  [A loss: 1.532699, acc: 0.000000]\n",
            "94: [D loss: 0.484613, acc: 0.851562]  [A loss: 1.185113, acc: 0.000000]\n",
            "95: [D loss: 0.586182, acc: 0.546875]  [A loss: 1.643384, acc: 0.000000]\n",
            "96: [D loss: 0.457553, acc: 0.835938]  [A loss: 1.229912, acc: 0.000000]\n",
            "97: [D loss: 0.583877, acc: 0.539062]  [A loss: 1.740433, acc: 0.000000]\n",
            "98: [D loss: 0.472393, acc: 0.867188]  [A loss: 1.165201, acc: 0.031250]\n",
            "99: [D loss: 0.584879, acc: 0.562500]  [A loss: 2.033365, acc: 0.000000]\n",
            "100: [D loss: 0.494878, acc: 0.890625]  [A loss: 0.814495, acc: 0.281250]\n",
            "101: [D loss: 0.724683, acc: 0.500000]  [A loss: 2.332452, acc: 0.000000]\n",
            "102: [D loss: 0.571765, acc: 0.757812]  [A loss: 0.545820, acc: 0.828125]\n",
            "103: [D loss: 0.853639, acc: 0.500000]  [A loss: 1.753460, acc: 0.000000]\n",
            "104: [D loss: 0.552423, acc: 0.742188]  [A loss: 0.780420, acc: 0.359375]\n",
            "105: [D loss: 0.663396, acc: 0.500000]  [A loss: 1.542251, acc: 0.000000]\n",
            "106: [D loss: 0.489894, acc: 0.890625]  [A loss: 0.955137, acc: 0.046875]\n",
            "107: [D loss: 0.571868, acc: 0.539062]  [A loss: 1.565390, acc: 0.000000]\n",
            "108: [D loss: 0.469109, acc: 0.828125]  [A loss: 1.158699, acc: 0.000000]\n",
            "109: [D loss: 0.573937, acc: 0.562500]  [A loss: 1.743732, acc: 0.000000]\n",
            "110: [D loss: 0.498445, acc: 0.804688]  [A loss: 1.066009, acc: 0.031250]\n",
            "111: [D loss: 0.590375, acc: 0.578125]  [A loss: 1.951667, acc: 0.000000]\n",
            "112: [D loss: 0.538940, acc: 0.781250]  [A loss: 0.627803, acc: 0.593750]\n",
            "113: [D loss: 0.805308, acc: 0.507812]  [A loss: 2.075817, acc: 0.000000]\n",
            "114: [D loss: 0.497357, acc: 0.812500]  [A loss: 0.660248, acc: 0.609375]\n",
            "115: [D loss: 0.752788, acc: 0.500000]  [A loss: 1.792502, acc: 0.000000]\n",
            "116: [D loss: 0.501877, acc: 0.843750]  [A loss: 0.908764, acc: 0.140625]\n",
            "117: [D loss: 0.563062, acc: 0.570312]  [A loss: 1.574138, acc: 0.000000]\n",
            "118: [D loss: 0.532093, acc: 0.734375]  [A loss: 1.063025, acc: 0.015625]\n",
            "119: [D loss: 0.599373, acc: 0.554688]  [A loss: 1.657894, acc: 0.000000]\n",
            "120: [D loss: 0.461221, acc: 0.882812]  [A loss: 1.062634, acc: 0.015625]\n",
            "121: [D loss: 0.565534, acc: 0.609375]  [A loss: 1.770367, acc: 0.000000]\n",
            "122: [D loss: 0.470781, acc: 0.828125]  [A loss: 0.932712, acc: 0.203125]\n",
            "123: [D loss: 0.623442, acc: 0.546875]  [A loss: 2.289445, acc: 0.000000]\n",
            "124: [D loss: 0.521778, acc: 0.750000]  [A loss: 0.568924, acc: 0.718750]\n",
            "125: [D loss: 0.843140, acc: 0.500000]  [A loss: 2.102946, acc: 0.000000]\n",
            "126: [D loss: 0.555031, acc: 0.742188]  [A loss: 0.685801, acc: 0.578125]\n",
            "127: [D loss: 0.712678, acc: 0.507812]  [A loss: 1.539267, acc: 0.000000]\n",
            "128: [D loss: 0.521172, acc: 0.812500]  [A loss: 1.119247, acc: 0.031250]\n",
            "129: [D loss: 0.571762, acc: 0.601562]  [A loss: 1.517437, acc: 0.000000]\n",
            "130: [D loss: 0.568137, acc: 0.671875]  [A loss: 1.386150, acc: 0.031250]\n",
            "131: [D loss: 0.553379, acc: 0.664062]  [A loss: 1.590055, acc: 0.000000]\n",
            "132: [D loss: 0.514403, acc: 0.820312]  [A loss: 1.130455, acc: 0.000000]\n",
            "133: [D loss: 0.650368, acc: 0.539062]  [A loss: 2.252723, acc: 0.000000]\n",
            "134: [D loss: 0.546755, acc: 0.789062]  [A loss: 0.550925, acc: 0.828125]\n",
            "135: [D loss: 0.805493, acc: 0.500000]  [A loss: 2.210297, acc: 0.000000]\n",
            "136: [D loss: 0.573184, acc: 0.648438]  [A loss: 0.617046, acc: 0.687500]\n",
            "137: [D loss: 0.785194, acc: 0.500000]  [A loss: 1.649256, acc: 0.000000]\n",
            "138: [D loss: 0.526447, acc: 0.828125]  [A loss: 0.956932, acc: 0.125000]\n",
            "139: [D loss: 0.628818, acc: 0.539062]  [A loss: 1.599599, acc: 0.000000]\n",
            "140: [D loss: 0.540215, acc: 0.773438]  [A loss: 1.042783, acc: 0.031250]\n",
            "141: [D loss: 0.621940, acc: 0.570312]  [A loss: 1.818297, acc: 0.000000]\n",
            "142: [D loss: 0.581207, acc: 0.781250]  [A loss: 0.737340, acc: 0.453125]\n",
            "143: [D loss: 0.782832, acc: 0.507812]  [A loss: 2.032146, acc: 0.000000]\n",
            "144: [D loss: 0.596328, acc: 0.695312]  [A loss: 0.630300, acc: 0.703125]\n",
            "145: [D loss: 0.784003, acc: 0.500000]  [A loss: 1.910094, acc: 0.000000]\n",
            "146: [D loss: 0.571928, acc: 0.750000]  [A loss: 0.882501, acc: 0.203125]\n",
            "147: [D loss: 0.670856, acc: 0.523438]  [A loss: 1.770229, acc: 0.000000]\n",
            "148: [D loss: 0.538628, acc: 0.812500]  [A loss: 0.983545, acc: 0.109375]\n",
            "149: [D loss: 0.659263, acc: 0.539062]  [A loss: 1.928944, acc: 0.000000]\n",
            "150: [D loss: 0.552456, acc: 0.773438]  [A loss: 0.824774, acc: 0.281250]\n",
            "151: [D loss: 0.724883, acc: 0.515625]  [A loss: 2.073759, acc: 0.000000]\n",
            "152: [D loss: 0.548719, acc: 0.750000]  [A loss: 0.694016, acc: 0.609375]\n",
            "153: [D loss: 0.756651, acc: 0.500000]  [A loss: 1.842651, acc: 0.000000]\n",
            "154: [D loss: 0.571113, acc: 0.710938]  [A loss: 0.729376, acc: 0.453125]\n",
            "155: [D loss: 0.718924, acc: 0.507812]  [A loss: 1.677343, acc: 0.000000]\n",
            "156: [D loss: 0.569327, acc: 0.781250]  [A loss: 0.910067, acc: 0.109375]\n",
            "157: [D loss: 0.674969, acc: 0.523438]  [A loss: 1.797421, acc: 0.000000]\n",
            "158: [D loss: 0.550884, acc: 0.781250]  [A loss: 0.937119, acc: 0.109375]\n",
            "159: [D loss: 0.704142, acc: 0.531250]  [A loss: 1.930985, acc: 0.000000]\n",
            "160: [D loss: 0.587485, acc: 0.726562]  [A loss: 0.676902, acc: 0.531250]\n",
            "161: [D loss: 0.799100, acc: 0.507812]  [A loss: 1.834659, acc: 0.000000]\n",
            "162: [D loss: 0.566934, acc: 0.757812]  [A loss: 0.802155, acc: 0.375000]\n",
            "163: [D loss: 0.703489, acc: 0.523438]  [A loss: 1.504501, acc: 0.000000]\n",
            "164: [D loss: 0.571048, acc: 0.757812]  [A loss: 0.964477, acc: 0.078125]\n",
            "165: [D loss: 0.669116, acc: 0.531250]  [A loss: 1.513353, acc: 0.000000]\n",
            "166: [D loss: 0.564284, acc: 0.781250]  [A loss: 0.873309, acc: 0.234375]\n",
            "167: [D loss: 0.660471, acc: 0.523438]  [A loss: 1.820422, acc: 0.000000]\n",
            "168: [D loss: 0.590278, acc: 0.750000]  [A loss: 0.649996, acc: 0.562500]\n",
            "169: [D loss: 0.753431, acc: 0.507812]  [A loss: 1.934490, acc: 0.000000]\n",
            "170: [D loss: 0.582870, acc: 0.703125]  [A loss: 0.616310, acc: 0.687500]\n",
            "171: [D loss: 0.767900, acc: 0.500000]  [A loss: 1.575021, acc: 0.000000]\n",
            "172: [D loss: 0.544984, acc: 0.843750]  [A loss: 0.759266, acc: 0.328125]\n",
            "173: [D loss: 0.709036, acc: 0.500000]  [A loss: 1.519704, acc: 0.000000]\n",
            "174: [D loss: 0.563832, acc: 0.812500]  [A loss: 0.930388, acc: 0.109375]\n",
            "175: [D loss: 0.611189, acc: 0.562500]  [A loss: 1.601349, acc: 0.000000]\n",
            "176: [D loss: 0.545476, acc: 0.789062]  [A loss: 1.037853, acc: 0.109375]\n",
            "177: [D loss: 0.639315, acc: 0.562500]  [A loss: 1.763887, acc: 0.000000]\n",
            "178: [D loss: 0.524048, acc: 0.875000]  [A loss: 0.787451, acc: 0.359375]\n",
            "179: [D loss: 0.681394, acc: 0.539062]  [A loss: 1.905270, acc: 0.000000]\n",
            "180: [D loss: 0.558474, acc: 0.796875]  [A loss: 0.713065, acc: 0.390625]\n",
            "181: [D loss: 0.740207, acc: 0.515625]  [A loss: 1.895280, acc: 0.000000]\n",
            "182: [D loss: 0.576809, acc: 0.742188]  [A loss: 0.668306, acc: 0.578125]\n",
            "183: [D loss: 0.699644, acc: 0.507812]  [A loss: 1.773854, acc: 0.000000]\n",
            "184: [D loss: 0.555771, acc: 0.750000]  [A loss: 0.717260, acc: 0.531250]\n",
            "185: [D loss: 0.710029, acc: 0.500000]  [A loss: 1.792973, acc: 0.000000]\n",
            "186: [D loss: 0.548907, acc: 0.828125]  [A loss: 0.722069, acc: 0.500000]\n",
            "187: [D loss: 0.683137, acc: 0.523438]  [A loss: 1.662737, acc: 0.000000]\n",
            "188: [D loss: 0.555697, acc: 0.781250]  [A loss: 0.780861, acc: 0.328125]\n",
            "189: [D loss: 0.656384, acc: 0.507812]  [A loss: 1.651653, acc: 0.000000]\n",
            "190: [D loss: 0.523089, acc: 0.851562]  [A loss: 0.747194, acc: 0.390625]\n",
            "191: [D loss: 0.668337, acc: 0.523438]  [A loss: 1.639635, acc: 0.000000]\n",
            "192: [D loss: 0.550359, acc: 0.835938]  [A loss: 0.904703, acc: 0.125000]\n",
            "193: [D loss: 0.639376, acc: 0.523438]  [A loss: 1.687121, acc: 0.000000]\n",
            "194: [D loss: 0.546764, acc: 0.804688]  [A loss: 0.873796, acc: 0.203125]\n",
            "195: [D loss: 0.685031, acc: 0.515625]  [A loss: 1.959142, acc: 0.000000]\n",
            "196: [D loss: 0.565054, acc: 0.773438]  [A loss: 0.639557, acc: 0.656250]\n",
            "197: [D loss: 0.744552, acc: 0.507812]  [A loss: 1.723322, acc: 0.000000]\n",
            "198: [D loss: 0.537207, acc: 0.820312]  [A loss: 0.791225, acc: 0.250000]\n",
            "199: [D loss: 0.720171, acc: 0.507812]  [A loss: 1.613726, acc: 0.000000]\n",
            "Elapsed: 40.34954380989075 sec \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAALECAYAAAACS1bEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}